{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b3fc4c0",
   "metadata": {},
   "source": [
    "# Attention and self attention mechanism\n",
    "Huilin Zhang hz3455@nyu.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8050ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f13cae",
   "metadata": {},
   "source": [
    "## 1. Code implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7186e7",
   "metadata": {},
   "source": [
    "### 1.1 Single head self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8cdfcd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_projection(x, weight, bias=None):\n",
    "    \"\"\"\n",
    "    Performs a linear projection.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor of shape (batch_size, seq_length, embed_dim).\n",
    "        weight: Weight matrix of shape (embed_dim, embed_dim).\n",
    "        bias: Bias vector of shape (embed_dim,).\n",
    "        \n",
    "    Returns:\n",
    "        Projected tensor of shape (batch_size, seq_length, embed_dim).\n",
    "    \"\"\"\n",
    "\n",
    "    if bias is not None: \n",
    "        return torch.matmul(x, weight) + bias \n",
    "    else:\n",
    "        return torch.matmul(x, weight)\n",
    "    \n",
    "def scaled_dot_product_attention(queries, keys, values, embed_dim):\n",
    "    \"\"\"\n",
    "    Computes scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        queries: Query tensor of shape (batch_size, seq_length, embed_dim).\n",
    "        keys: Key tensor of shape (batch_size, seq_length, embed_dim).\n",
    "        values: Value tensor of shape (batch_size, seq_length, embed_dim).\n",
    "        embed_dim: Dimension of the embedding.\n",
    "        \n",
    "    Returns:\n",
    "        attended_values: Attention output of shape (batch_size, seq_length, embed_dim).\n",
    "        attention_weights: Attention weights of shape (batch_size, seq_length, seq_length).\n",
    "    \"\"\"\n",
    "    scores = torch.matmul(queries, keys.transpose(-2, -1)) / (embed_dim ** 0.5)\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attention_weights, values)\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e476a0f",
   "metadata": {},
   "source": [
    "### 1.2 Single head self attention example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "16fe6425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5525,  0.6355, -0.3968, -0.6571],\n",
       "         [-1.6428,  0.9803, -0.0421, -0.8206],\n",
       "         [ 0.3133, -1.1352,  0.3773, -0.2824]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1 \n",
    "seq_length = 3 #sentence length\n",
    "embed_dim = 4 #512 in paper\n",
    "\n",
    "#set a random seed\n",
    "#torch.manual_seed(seed=1) \n",
    "# Create random input tensor (batch_size, seq_length, embed_dim)\n",
    "x = torch.randn(batch_size, seq_length, embed_dim)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53d15dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and biases for linear projections\n",
    "query_weight = torch.randn(embed_dim, embed_dim)\n",
    "key_weight = torch.randn(embed_dim, embed_dim)\n",
    "value_weight = torch.randn(embed_dim, embed_dim)\n",
    "query_bias = torch.randn(embed_dim)\n",
    "key_bias = torch.randn(embed_dim)\n",
    "value_bias = torch.randn(embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dba4778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute queries, keys, and values using linear projections\n",
    "queries = linear_projection(x, query_weight, query_bias)\n",
    "keys = linear_projection(x, key_weight, key_bias)\n",
    "values = linear_projection(x, value_weight, value_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "53446ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaled dot-product attention\n",
    "output = scaled_dot_product_attention(queries, keys, values, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ef12ce29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[[ 0.6614,  0.2669,  0.0617,  0.6213],\n",
      "         [-0.4519, -0.1661, -1.5228,  0.3817],\n",
      "         [-1.0276, -0.5631, -0.8923, -0.0583]]])\n",
      "Output\n",
      "tensor([[[-1.6712,  0.3590,  0.9510,  1.1844],\n",
      "         [-2.1862,  0.0148,  1.9578,  1.9829],\n",
      "         [-2.1960,  0.0468,  1.8642,  1.9638]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(\"Output\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3b92ec",
   "metadata": {},
   "source": [
    "### 1.3 multi head self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c179299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_projection(x, weight, bias=None):\n",
    "    \"\"\"\n",
    "    Performs a linear projection.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor of shape (batch_size, seq_length, embed_dim).\n",
    "        weight: Weight matrix of shape (embed_dim, embed_dim).\n",
    "        bias: Bias vector of shape (embed_dim,).\n",
    "        \n",
    "    Returns:\n",
    "        Projected tensor of shape (batch_size, seq_length, embed_dim).\n",
    "    \"\"\"\n",
    "\n",
    "    if bias is not None: \n",
    "        return torch.matmul(x, weight) + bias \n",
    "    else:\n",
    "        return torch.matmul(x, weight)\n",
    "    \n",
    "\n",
    "def scaled_dot_product_attention(queries, keys, values, embed_dim):\n",
    "    \"\"\"\n",
    "    Computes scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        queries: Query tensor of shape (batch_size, seq_length, embed_dim).\n",
    "        keys: Key tensor of shape (batch_size, seq_length, embed_dim).\n",
    "        values: Value tensor of shape (batch_size, seq_length, embed_dim).\n",
    "        embed_dim: Dimension of the embedding.\n",
    "        \n",
    "    Returns:\n",
    "        attended_values: Attention output of shape (batch_size, seq_length, embed_dim).\n",
    "        attention_weights: Attention weights of shape (batch_size, seq_length, seq_length).\n",
    "    \"\"\"\n",
    "    scores = torch.matmul(queries, keys.transpose(-2, -1)) / (embed_dim ** 0.5)\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attention_weights, values)\n",
    "    return output,attention_weights\n",
    "\n",
    "def create_parameters(embed_dim, num_heads):\n",
    "    # Since we don't have the pretrained parameters here, we use torch.randn to create random parameters\n",
    "    query_weights = [torch.randn(embed_dim, embed_dim) for _ in range(num_heads)]\n",
    "    key_weights = [torch.randn(embed_dim, embed_dim) for _ in range(num_heads)]\n",
    "    value_weights = [torch.randn(embed_dim, embed_dim) for _ in range(num_heads)]\n",
    "    query_biases = [torch.randn(embed_dim) for _ in range(num_heads)]\n",
    "    key_biases = [torch.randn(embed_dim) for _ in range(num_heads)]\n",
    "    value_biases = [torch.randn(embed_dim) for _ in range(num_heads)]\n",
    "    \n",
    "    # Stack the weights and biases for easy access\n",
    "    query_weights = torch.stack(query_weights)\n",
    "    key_weights = torch.stack(key_weights)\n",
    "    value_weights = torch.stack(value_weights)\n",
    "    query_biases = torch.stack(query_biases)\n",
    "    key_biases = torch.stack(key_biases)\n",
    "    value_biases = torch.stack(value_biases)\n",
    "    \n",
    "    w0 = torch.randn(embed_dim*num_heads, embed_dim)\n",
    "\n",
    "    return w0,query_weights, query_biases, key_weights, key_biases, value_weights, value_biases\n",
    "\n",
    "\n",
    "\n",
    "def multi_head_attention(x, w0,num_heads, query_weights, query_biases, key_weights, key_biases, value_weights, value_biases):\n",
    "    \"\"\"\n",
    "    Performs multi-head self-attention.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor of shape (batch_size, seq_length, embed_dim).\n",
    "        num_heads: Number of attention heads.\n",
    "        query_weights: Query weights tensor of shape (num_heads, embed_dim, embed_dim).\n",
    "        query_biases: Query biases tensor of shape (num_heads, embed_dim).\n",
    "        key_weights: Key weights tensor of shape (num_heads, embed_dim, embed_dim).\n",
    "        key_biases: Key biases tensor of shape (num_heads, embed_dim).\n",
    "        value_weights: Value weights tensor of shape (num_heads, embed_dim, embed_dim).\n",
    "        value_biases: Value biases tensor of shape (num_heads, embed_dim).\n",
    "        \n",
    "    Returns:\n",
    "        output: Attention output of shape (batch_size, seq_length, embed_dim).\n",
    "        attention_weights: Attention weights of shape (batch_size, num_heads, seq_length, seq_length).\n",
    "    \"\"\"\n",
    "    batch_size, seq_length, embed_dim = x.size()\n",
    "    \n",
    "    # Compute queries, keys, and values for each head\n",
    "    queries = [linear_projection(x, query_weights[i], query_biases[i]) for i in range(num_heads)]\n",
    "    keys = [linear_projection(x, key_weights[i], key_biases[i]) for i in range(num_heads)]\n",
    "    values = [linear_projection(x, value_weights[i], value_biases[i]) for i in range(num_heads)]\n",
    "    \n",
    "    # Apply scaled dot-product attention for each head\n",
    "    attention_outputs = []\n",
    "    attention_weights = []\n",
    "    for i in range(num_heads):\n",
    "        head_attention_output, head_attention_weights = scaled_dot_product_attention(queries[i], keys[i], values[i], embed_dim)\n",
    "        attention_outputs.append(head_attention_output)\n",
    "        attention_weights.append(head_attention_weights)\n",
    "#    print ('check',attention_outputs)\n",
    "# Concatenate the attention outputs from all heads\n",
    "    concatenated_attention_outputs = torch.cat(attention_outputs, dim=-1)\n",
    "#    print(concatenated_attention_outputs.shape)\n",
    "\n",
    "# Apply a linear transformation to the concatenated attention outputs\n",
    "#    w0 = torch.randn(concatenated_attention_outputs.size(-1), embed_dim)\n",
    "    output = torch.matmul(concatenated_attention_outputs, w0)\n",
    "#    print('check',w0)\n",
    "# Stack the attention weights from all heads\n",
    "    attention_weights = torch.stack(attention_weights, dim=1)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d665f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \n",
    "batch_size = 1\n",
    "seq_length = 1 #3个词\n",
    "embed_dim = 5  # 2特征\n",
    "num_heads = 2 \n",
    "\n",
    "\n",
    "# Create random input tensor (batch_size, seq_length, embed_dim)\n",
    "x = torch.randn(batch_size, seq_length, embed_dim)\n",
    "w0,query_weights, query_biases, key_weights, key_biases, value_weights, value_biases=create_parameters(\n",
    "    embed_dim, num_heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "92b23a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[[-0.1977,  0.0294,  0.4781,  0.1980,  0.2429]]])\n",
      "\n",
      "Output:\n",
      "tensor([[[-6.9007,  8.6663, -0.8672,  5.1374, -4.9482]]])\n"
     ]
    }
   ],
   "source": [
    "# Apply multi-head self-attention\n",
    "attended_values, attention_weights = multi_head_attention(\n",
    "    x, w0,num_heads, query_weights, query_biases, key_weights, key_biases, value_weights, value_biases)\n",
    "\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(\"\\nOutput:\")\n",
    "print(attended_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c732317e",
   "metadata": {},
   "source": [
    "### 1.4 Use the number in the hands on example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "98ba3b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[[1., 2., 3.]]])\n",
      "Output:\n",
      "tensor([[[3., 5., 4.]]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[[1, 2, 3]]], dtype=torch.float32)\n",
    "print('Input:')\n",
    "print(X)\n",
    "def create_parameters():\n",
    "    WQ1 = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=torch.float32)\n",
    "    WK1 = torch.tensor([[0, 1, 0], [0, 0, 1], [1, 0, 0]], dtype=torch.float32)\n",
    "    WV1 = torch.tensor([[0, 0, 1], [1, 0, 0], [0, 1, 0]], dtype=torch.float32)\n",
    "    \n",
    "    WQ2 = torch.tensor([[0, 1, 0], [0, 0, 1], [1, 0, 0]], dtype=torch.float32)\n",
    "    WK2 = torch.tensor([[0, 0, 1], [1, 0, 0], [0, 1, 0]], dtype=torch.float32)\n",
    "    WV2 = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=torch.float32)\n",
    "    \n",
    "    query_weights = torch.stack([WQ1, WQ2])\n",
    "    key_weights = torch.stack([WK1, WK2])\n",
    "    value_weights = torch.stack([WV1, WV2])\n",
    "    \n",
    "    query_biases = torch.zeros(2, 3)\n",
    "    key_biases = torch.zeros(2, 3)\n",
    "    value_biases = torch.zeros(2, 3)\n",
    "    \n",
    "    return query_weights, query_biases, key_weights, key_biases, value_weights, value_biases\n",
    "\n",
    "num_heads = 2\n",
    "query_weights, query_biases, key_weights, key_biases, value_weights, value_biases = create_parameters()\n",
    "\n",
    "W0 = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=torch.float32)\n",
    "\n",
    "output, attention_weights = multi_head_attention(\n",
    "    X, W0, num_heads, query_weights, query_biases, key_weights, key_biases, value_weights, value_biases)\n",
    "print('Output:')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbec2a2",
   "metadata": {},
   "source": [
    "# 2. Multi head hands on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ad248044",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "\n",
    "#parameters \n",
    "WQ1 = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=torch.float32)\n",
    "WK1 = torch.tensor([[0, 1, 0], [0, 0, 1], [1, 0, 0]], dtype=torch.float32)\n",
    "WV1 = torch.tensor([[0, 0, 1], [1, 0, 0], [0, 1, 0]], dtype=torch.float32)\n",
    "\n",
    "WQ2 = torch.tensor([[0, 1, 0], [0, 0, 1], [1, 0, 0]], dtype=torch.float32)\n",
    "WK2 = torch.tensor([[0, 0, 1], [1, 0, 0], [0, 1, 0]], dtype=torch.float32)\n",
    "WV2 = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=torch.float32)\n",
    "\n",
    "\n",
    "# compute Q,K,V\n",
    "Q1 = torch.matmul(X, WQ1)\n",
    "K1 = torch.matmul(X, WK1)\n",
    "V1 = torch.matmul(X, WV1)\n",
    "\n",
    "Q2 = torch.matmul(X, WQ2)\n",
    "K2 = torch.matmul(X, WK2)\n",
    "V2 = torch.matmul(X, WV2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "309056f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3.]), tensor([3., 1., 2.]))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1,Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "77f0e961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3., 1., 2.]), tensor([2., 3., 1.]))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K1,K2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "092da16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 3., 1.]), tensor([1., 2., 3.]))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V1,V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1625770c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[11.]]), tensor([[11.]]))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Attention score\n",
    "score1 = torch.matmul(Q1.unsqueeze(0), K1.unsqueeze(1))\n",
    "score2 = torch.matmul(Q2.unsqueeze(0), K2.unsqueeze(1))\n",
    "score1,score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c565212d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[6.3509]]), tensor([[6.3509]]))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaled\n",
    "dk = K1.size(-1)\n",
    "scaled_score1 = score1 / torch.sqrt(torch.tensor(dk, dtype=torch.float32))\n",
    "scaled_score2 = score2 / torch.sqrt(torch.tensor(dk, dtype=torch.float32))\n",
    "scaled_score1 ,scaled_score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2634090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.]]), tensor([[1.]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the weight\n",
    "weight1 = F.softmax(scaled_score1, dim=-1)\n",
    "weight2 = F.softmax(scaled_score2, dim=-1)\n",
    "weight1,weight2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "07e4927b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 3., 1.]), tensor([1., 2., 3.]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted sum\n",
    "output1 = torch.matmul(weight1, V1.unsqueeze(0)).squeeze(0)\n",
    "output2 = torch.matmul(weight2, V2.unsqueeze(0)).squeeze(0)\n",
    "output1,output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10fdb640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3., 1., 1., 2., 3.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate\n",
    "concat_output = torch.cat((output1, output2), dim=-1)\n",
    "concat_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ef58924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 5., 4.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final linear transformation\n",
    "W0 = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=torch.float32)\n",
    "final_output = torch.matmul(concat_output.unsqueeze(0), W0).squeeze(0)\n",
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53fb00a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

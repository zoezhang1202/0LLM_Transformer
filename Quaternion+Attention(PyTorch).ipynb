{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09e8d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88ea74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_quaternion_mul(kernel, concat_dim=0):\n",
    "    r, i, j, k = torch.chunk(kernel, 4, dim=-1)\n",
    "    r2 = torch.cat([r, -i, -j, -k], dim=-1)\n",
    "    i2 = torch.cat([i, r, -k, j], dim=-1)\n",
    "    j2 = torch.cat([j, k, r, -i], dim=-1)\n",
    "    k2 = torch.cat([k, -j, i, r], dim=-1)\n",
    "    hamilton = torch.cat([r2, i2, j2, k2], dim=concat_dim)\n",
    "    return hamilton\n",
    "\n",
    "def get_r(x, dim=1):\n",
    "    return torch.chunk(x, 4, dim=dim)[0]\n",
    "\n",
    "def get_i(x, dim=1):\n",
    "    return torch.chunk(x, 4, dim=dim)[1]\n",
    "\n",
    "def get_j(x, dim=1):\n",
    "    return torch.chunk(x, 4, dim=dim)[2]\n",
    "\n",
    "def get_k(x, dim=1):\n",
    "    return torch.chunk(x, 4, dim=dim)[3]\n",
    "\n",
    "def quaternion_attention(a, b):\n",
    "    \"\"\"Performs dot product attention between two quaternion sequences.\"\"\"\n",
    "    print(\"light Attention!\")\n",
    "    print(a)\n",
    "    print(b)\n",
    "    \n",
    "    ar, ai, aj, ak = torch.chunk(a, 4, dim=-1)\n",
    "    br, bi, bj, bk = torch.chunk(b, 4, dim=-1)\n",
    "    \n",
    "    r = torch.matmul(ar, br.transpose(-2, -1)) - torch.matmul(ai, bi.transpose(-2, -1)) - torch.matmul(aj, bj.transpose(-2, -1)) - torch.matmul(ak, bk.transpose(-2, -1))\n",
    "    i = torch.matmul(ar, bi.transpose(-2, -1)) + torch.matmul(ai, br.transpose(-2, -1)) + torch.matmul(aj, bk.transpose(-2, -1)) - torch.matmul(ak, bj.transpose(-2, -1))\n",
    "    j = torch.matmul(ar, bj.transpose(-2, -1)) - torch.matmul(ai, bk.transpose(-2, -1)) + torch.matmul(aj, br.transpose(-2, -1)) + torch.matmul(ak, bi.transpose(-2, -1))\n",
    "    k = torch.matmul(ar, bk.transpose(-2, -1)) + torch.matmul(ai, bj.transpose(-2, -1)) - torch.matmul(aj, bi.transpose(-2, -1)) + torch.matmul(ak, br.transpose(-2, -1))\n",
    "    \n",
    "    return [r, i, j, k]\n",
    "\n",
    "def quaternion_dot_product_att(a, b):\n",
    "    \"\"\"Wrapper for two sequences.\"\"\"\n",
    "    al = a.shape[1]\n",
    "    bl = b.shape[1]\n",
    "    d = a.shape[2]\n",
    "    bsz = b.shape[0]\n",
    "    \n",
    "    a = a.view(-1, d)\n",
    "    a = a.repeat(bl, 1)\n",
    "    b = b.view(-1, d)\n",
    "    b = b.repeat(al, 1)\n",
    "    \n",
    "    att = quaternion_dot(a, b)\n",
    "    att = att.view(bsz, -1, al * bl)\n",
    "    att = torch.sum(att, dim=1)\n",
    "    \n",
    "    return att.view(-1, al * bl)\n",
    "\n",
    "def quaternion_dot_3d(q0, q1):\n",
    "    d = q0.shape[2]\n",
    "    sq = q0.shape[1]\n",
    "    \n",
    "    q0 = q0.view(-1, d)\n",
    "    q1 = q1.view(-1, d)\n",
    "    \n",
    "    out = quaternion_dot(q0, q1)\n",
    "    return out.view(-1, sq, d)\n",
    "\n",
    "def quaternion_dot(q0, q1):\n",
    "    \"\"\"Quaternion product between 2 quaternions.\"\"\"\n",
    "    q1_r = get_r(q1)\n",
    "    q1_i = get_i(q1)\n",
    "    q1_j = get_j(q1)\n",
    "    q1_k = get_k(q1)\n",
    "    \n",
    "    r_base = q0 * q1\n",
    "    r = get_r(r_base) - get_i(r_base) - get_j(r_base) - get_k(r_base)\n",
    "    \n",
    "    i_base = q0 * torch.cat([q1_i, q1_r, q1_k, q1_j], dim=1)\n",
    "    i = get_r(i_base) + get_i(i_base) + get_j(i_base) - get_k(i_base)\n",
    "    \n",
    "    j_base = q0 * torch.cat([q1_j, q1_k, q1_r, q1_i], dim=1)\n",
    "    j = get_r(j_base) - get_i(j_base) + get_j(j_base) + get_k(j_base)\n",
    "    \n",
    "    k_base = q0 * torch.cat([q1_k, q1_j, q1_i, q1_r], dim=1)\n",
    "    k = get_r(k_base) + get_i(k_base) - get_j(k_base) + get_k(k_base)\n",
    "    \n",
    "    return torch.cat([r, i, j, k], dim=1)\n",
    "\n",
    "def quaternion_concat(x, dim):\n",
    "    \"\"\"Concatenates quaternion components individually.\"\"\"\n",
    "    output = [[] for _ in range(4)]\n",
    "    for _x in x:\n",
    "        sp = torch.chunk(_x, 4, dim=dim)\n",
    "        for i in range(4):\n",
    "            output[i].append(sp[i])\n",
    "    \n",
    "    final = []\n",
    "    for o in output:\n",
    "        o = torch.cat(o, dim)\n",
    "        final.append(o)\n",
    "    \n",
    "    return torch.cat(final, dim)\n",
    "\n",
    "def quaternion_ffn_3d(x, dim, num_layers=1, activation=None):\n",
    "    \"\"\"Quaternion Feed-forward layers to 3D input [bsz x seq_len x dim].\"\"\"\n",
    "    print(\"QFFN layer..\")\n",
    "    _d = x.shape[2]\n",
    "    sq = x.shape[1]\n",
    "    \n",
    "    x = x.view(-1, _d)\n",
    "    x = quaternion_ffn(x, dim, num_layers=num_layers, activation=activation)\n",
    "    return x.view(-1, sq, dim)\n",
    "\n",
    "def quaternion_ffn(x, dim, num_layers=1, activation=None):\n",
    "    \"\"\"Implements quaternion feed-forward layer.\"\"\"\n",
    "    input_dim = x.shape[1] // 4\n",
    "    kernel = torch.nn.Parameter(torch.randn(input_dim, dim))\n",
    "    hamilton = make_quaternion_mul(kernel)\n",
    "    \n",
    "    output = torch.matmul(x, hamilton)\n",
    "    if activation:\n",
    "        output = activation(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def hamilton_product(x, kernel):\n",
    "    h = make_quaternion_mul(kernel)\n",
    "    return torch.matmul(x, h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4702cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a28e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_component(antecedent,\n",
    "                                total_depth,\n",
    "                                filter_width=1,\n",
    "                                padding=\"valid\",\n",
    "                                name=\"c\",\n",
    "                                vars_3d_num_heads=0):\n",
    "    input_depth = antecedent.size(-1)\n",
    "    initializer_stddev = input_depth ** -0.5\n",
    "    if \"q\" in name:\n",
    "        depth_per_head = total_depth\n",
    "        initializer_stddev *= depth_per_head ** -0.5\n",
    "\n",
    "    if vars_3d_num_heads > 0:\n",
    "        assert filter_width == 1\n",
    "        input_depth = antecedent.size(-1)\n",
    "        depth_per_head = total_depth // vars_3d_num_heads\n",
    "        initializer_stddev = input_depth ** -0.5\n",
    "        if \"q\" in name:\n",
    "            initializer_stddev *= depth_per_head ** -0.5\n",
    "        var = nn.Parameter(torch.randn(input_depth, vars_3d_num_heads, total_depth // vars_3d_num_heads) * initializer_stddev)\n",
    "        var = var.to(antecedent.dtype)\n",
    "        var = var.view(input_depth, total_depth)\n",
    "        return torch.einsum('bld,df->blf', antecedent, var)\n",
    "    \n",
    "    if filter_width == 1:\n",
    "        return quarternion_ffn_3d(antecedent, total_depth, name=name,\n",
    "                                  init=torch.nn.init.normal_(torch.empty(input_depth, total_depth), mean=0, std=initializer_stddev))\n",
    "    else:\n",
    "        return F.conv1d(antecedent.permute(0, 2, 1), \n",
    "                        torch.randn(total_depth, input_depth, filter_width) * initializer_stddev, \n",
    "                        padding=padding).permute(0, 2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_qkv(query_antecedent,\n",
    "                memory_antecedent,\n",
    "                total_key_depth,\n",
    "                total_value_depth,\n",
    "                q_filter_width=1,\n",
    "                kv_filter_width=1,\n",
    "                q_padding=\"valid\",\n",
    "                kv_padding=\"valid\",\n",
    "                vars_3d_num_heads=0):\n",
    "    if memory_antecedent is None:\n",
    "        memory_antecedent = query_antecedent\n",
    "    q = compute_attention_component(\n",
    "        query_antecedent,\n",
    "        total_key_depth,\n",
    "        q_filter_width,\n",
    "        q_padding,\n",
    "        \"q\",\n",
    "        vars_3d_num_heads=vars_3d_num_heads)\n",
    "    k = compute_attention_component(\n",
    "        memory_antecedent,\n",
    "        total_key_depth,\n",
    "        kv_filter_width,\n",
    "        kv_padding,\n",
    "        \"k\",\n",
    "        vars_3d_num_heads=vars_3d_num_heads)\n",
    "    v = compute_attention_component(\n",
    "        memory_antecedent,\n",
    "        total_value_depth,\n",
    "        kv_filter_width,\n",
    "        kv_padding,\n",
    "        \"v\",\n",
    "        vars_3d_num_heads=vars_3d_num_heads)\n",
    "    return q, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, num_heads):\n",
    "    batch_size, length, depth = x.size()\n",
    "    depth_per_head = depth // num_heads\n",
    "    x = x.view(batch_size, length, num_heads, depth_per_head)\n",
    "    return x.permute(0, 2, 1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f243ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_heads(x):\n",
    "    batch_size, num_heads, length, depth_per_head = x.size()\n",
    "    return x.permute(0, 2, 1, 3).contiguous().view(batch_size, length, num_heads * depth_per_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d31c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention(q, k, v, bias, dropout_rate=0.0, image_shapes=None,\n",
    "                          save_weights_to=None, make_image_summary=True,\n",
    "                          dropout_broadcast_dims=None):\n",
    "    logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    if bias is not None:\n",
    "        logits += bias\n",
    "    weights = F.softmax(logits, dim=-1)\n",
    "    if dropout_rate > 0.0:\n",
    "        weights = F.dropout(weights, p=dropout_rate)\n",
    "    return torch.matmul(weights, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823fc16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_dot_product_attention(q, k, v, bias, dropout_rate=0.0, image_shapes=None,\n",
    "                                     save_weights_to=None, make_image_summary=True,\n",
    "                                     dropout_broadcast_dims=None):\n",
    "    print(\"Using QDP attention..\")\n",
    "    logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    if bias is not None:\n",
    "        logits += bias\n",
    "    weights = F.softmax(logits, dim=-1)\n",
    "    if dropout_rate > 0.0:\n",
    "        weights = F.dropout(weights, p=dropout_rate)\n",
    "    return torch.matmul(weights, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f0ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def quarternion_ffn_3d(x, output_depth, name='output_transform', init=None):\n",
    "#    return F.linear(x, init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416e8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a6950ce",
   "metadata": {},
   "source": [
    "# list for what we had so far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc886ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(query_antecedent,\n",
    "                        memory_antecedent,\n",
    "                        bias,\n",
    "                        total_key_depth,\n",
    "                        total_value_depth,\n",
    "                        output_depth,\n",
    "                        num_heads,\n",
    "                        dropout_rate,\n",
    "                        attention_type=\"dot_product\",\n",
    "                        max_relative_position=None,\n",
    "                        heads_share_relative_embedding=False,\n",
    "                        add_relative_to_values=False,\n",
    "                        image_shapes=None,\n",
    "                        block_length=128,\n",
    "                        block_width=128,\n",
    "                        q_filter_width=1,\n",
    "                        kv_filter_width=1,\n",
    "                        q_padding=\"valid\",\n",
    "                        kv_padding=\"valid\",\n",
    "                        cache=None,\n",
    "                        gap_size=0,\n",
    "                        num_memory_blocks=2,\n",
    "                        name=\"multihead_attention\",\n",
    "                        save_weights_to=None,\n",
    "                        make_image_summary=True,\n",
    "                        dropout_broadcast_dims=None,\n",
    "                        vars_3d=False,\n",
    "                        is_training=False,\n",
    "                        **kwargs):\n",
    "    if total_key_depth % num_heads != 0:\n",
    "        raise ValueError(\"Key depth (%d) must be divisible by the number of attention heads (%d).\" % (total_key_depth, num_heads))\n",
    "    if total_value_depth % num_heads != 0:\n",
    "        raise ValueError(\"Value depth (%d) must be divisible by the number of attention heads (%d).\" % (total_value_depth, num_heads))\n",
    "\n",
    "    vars_3d_num_heads = num_heads if vars_3d else 0\n",
    "    \n",
    "    if cache is None or memory_antecedent is None:\n",
    "        q, k, v = compute_qkv(query_antecedent, memory_antecedent,\n",
    "                              total_key_depth, total_value_depth, q_filter_width,\n",
    "                              kv_filter_width, q_padding, kv_padding,\n",
    "                              vars_3d_num_heads=vars_3d_num_heads)\n",
    "    if cache is not None:\n",
    "        if attention_type not in [\"dot_product\", \"dot_product_relative\", \"quaternion_dot_product\"]:\n",
    "            raise NotImplementedError(\"Caching is not guaranteed to work with attention types other than dot_product.\")\n",
    "        if bias is None:\n",
    "            raise ValueError(\"Bias required for caching. See function docstring for details.\")\n",
    "\n",
    "        if memory_antecedent is not None:\n",
    "            q = compute_attention_component(query_antecedent, total_key_depth,\n",
    "                                            q_filter_width, q_padding, \"q\",\n",
    "                                            vars_3d_num_heads=vars_3d_num_heads)\n",
    "            k = cache[\"k_encdec\"]\n",
    "            v = cache[\"v_encdec\"]\n",
    "        else:\n",
    "            k = split_heads(k, num_heads)\n",
    "            v = split_heads(v, num_heads)\n",
    "            decode_loop_step = kwargs.get(\"decode_loop_step\")\n",
    "            if decode_loop_step is None:\n",
    "                k = cache[\"k\"] = torch.cat([cache[\"k\"], k], dim=2)\n",
    "                v = cache[\"v\"] = torch.cat([cache[\"v\"], v], dim=2)\n",
    "            else:\n",
    "                tmp_k = cache[\"k\"].permute(2, 0, 1, 3)\n",
    "                tmp_k[decode_loop_step] = k.squeeze(2)\n",
    "                k = cache[\"k\"] = tmp_k.permute(1, 2, 0, 3)\n",
    "                tmp_v = cache[\"v\"].permute(2, 0, 1, 3)\n",
    "                tmp_v[decode_loop_step] = v.squeeze(2)\n",
    "                v = cache[\"v\"] = tmp_v.permute(1, 2, 0, 3)\n",
    "\n",
    "    q = split_heads(q, num_heads)\n",
    "    if cache is None:\n",
    "        k = split_heads(k, num_heads)\n",
    "        v = split_heads(v, num_heads)\n",
    "\n",
    "    key_depth_per_head = total_key_depth // num_heads\n",
    "    if not vars_3d:\n",
    "        q *= key_depth_per_head**-0.5\n",
    "\n",
    "    additional_returned_value = None\n",
    "    if callable(attention_type):\n",
    "        x = attention_type(q, k, v, **kwargs)\n",
    "        if isinstance(x, tuple):\n",
    "            x, additional_returned_value = x\n",
    "    elif attention_type == \"dot_product\":\n",
    "        x = dot_product_attention(q, k, v, bias, dropout_rate, image_shapes,\n",
    "                                  save_weights_to=save_weights_to,\n",
    "                                  make_image_summary=make_image_summary,\n",
    "                                  dropout_broadcast_dims=dropout_broadcast_dims)\n",
    "    elif attention_type == 'quaternion_dot_product':\n",
    "        print(\"Using QDP attention..\")\n",
    "        x = quaternion_dot_product_attention(q, k, v, bias, dropout_rate, image_shapes,\n",
    "                                             save_weights_to=save_weights_to,\n",
    "                                             make_image_summary=make_image_summary,\n",
    "                                             dropout_broadcast_dims=dropout_broadcast_dims)\n",
    "\n",
    "    x = combine_heads(x)\n",
    "    x = quarternion_ffn_3d(x, output_depth, name='output_transform',\n",
    "                           init=torch.nn.init.normal_(torch.empty(output_depth, output_depth), mean=0, std=output_depth ** -0.5))\n",
    "\n",
    "    if additional_returned_value is not None:\n",
    "        return x, additional_returned_value\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd41240e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using QDP attention..\n",
      "Using QDP attention..\n",
      "torch.Size([1, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Testing the updated multihead_attention function\n",
    "query_antecedent = torch.randn(1, 2, 4)\n",
    "memory_antecedent = torch.randn(1, 8, 4)\n",
    "bias = None\n",
    "total_key_depth = 4\n",
    "total_value_depth = 4\n",
    "output_depth = 4\n",
    "num_heads = 2\n",
    "dropout_rate = 0.1\n",
    "attention_type = \"quaternion_dot_product\"\n",
    "vars_3d = False\n",
    "\n",
    "output = multihead_attention(query_antecedent, memory_antecedent, bias,\n",
    "                             total_key_depth, total_value_depth, output_depth,\n",
    "                             num_heads, dropout_rate, attention_type, vars_3d=vars_3d)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5081b4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2630, -0.1027, -0.0266, -0.0309],\n",
       "         [ 0.3399,  0.1480,  0.0696,  0.0332]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574a027a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c332fff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0612"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f87d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LshGating:\n",
    "    \"\"\"Class to split key/queries into separate buckets.\"\"\"\n",
    "\n",
    "    def __init__(self, depth, nb_hyperplanes, nb_replicat=1, trainable=False):\n",
    "        \"\"\"Construct the gating function parameters.\n",
    "\n",
    "        Compute the gates for a single head.\n",
    "\n",
    "        Args:\n",
    "            depth (int): Dimension of the key/queries to dispatch\n",
    "            nb_hyperplanes (int): Nb of vectors use to split the space. Will determine\n",
    "                the number of buckets (2^nb_hyperplanes - 1).\n",
    "            nb_replicat (int): Redundancy to avoid the edge cases (to be in one bucket\n",
    "                the input should be in a majority)\n",
    "            trainable (bool): If True, a balance loss is added to force the hyperplane\n",
    "                to divide the key/query space evenly\n",
    "        \"\"\"\n",
    "        self.depth = depth\n",
    "        self.nb_hyperplanes = nb_hyperplanes\n",
    "        self.nb_buckets = 2**nb_hyperplanes\n",
    "        self.nb_replicat = nb_replicat  # Unused for now\n",
    "        self.trainable = trainable  # Unused for now\n",
    "\n",
    "        self.dispatchers = {}\n",
    "\n",
    "        assert self.nb_replicat == 1  # For now\n",
    "\n",
    "        # Vectors defining the hyperplanes\n",
    "        self.t_vectors = nn.Parameter(\n",
    "            torch.randn(self.depth, self.nb_hyperplanes * self.nb_replicat),\n",
    "            requires_grad=self.trainable\n",
    "        )\n",
    "\n",
    "        # Projection vector from the bit space to similarity score space\n",
    "        self.t_group = torch.tensor(\n",
    "            [self._idx_to_bits(i) for i in range(self.nb_buckets)],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    def _idx_to_bits(self, i):\n",
    "        \"\"\"Convert a group index to its bit representation.\"\"\"\n",
    "        bits = bin(i)[2:].zfill(self.nb_hyperplanes)  # Pad the bits str with 0\n",
    "        return [-1.0 if b == \"0\" else 1.0 for b in bits]\n",
    "\n",
    "    def get_gates(self, x):\n",
    "        \"\"\"Return the bucket id of the given tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): float32 of shape [length, depth]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: One-hot vector int64 of shape [length, nb_buckets]\n",
    "                containing the id of the bucket\n",
    "        \"\"\"\n",
    "        # The balance loss doesn't propagate to the rest of the network\n",
    "        x = x.detach()\n",
    "        # [length, depth] * [depth, nb_vectors * replicat]\n",
    "        x = torch.matmul(x, self.t_vectors)\n",
    "        # [length, nb_vector * replicat]\n",
    "        x = torch.sign(x)  # Get on which side of the hyperplane the keys are.\n",
    "\n",
    "        # x = torch.reshape(x, [-1, nb_replicat, nb_vector])\n",
    "        # [length, replicat, nb_vector] * [nb_vector, 2^nb_vector - 1]\n",
    "\n",
    "        x = torch.matmul(x, self.t_group.T) / self.nb_hyperplanes\n",
    "        # We get a similarity score for each of the group between [-1, 1]\n",
    "        # [length, (replicat,) 2^nb_vector - 1]\n",
    "        # Do an argmax to get the most likely group for each replicat\n",
    "        x = torch.argmax(x, dim=-1)\n",
    "        # [length(, replicat)]\n",
    "        # One-hot for compatibility with the sparse dispatcher\n",
    "        x = F.one_hot(x, num_classes=self.nb_buckets).float()\n",
    "        # TODO: Use a loss to force an even distribution\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ff2a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_to_padding(emb):\n",
    "    \"\"\"Calculates the padding mask based on which embeddings are all zero.\n",
    "\n",
    "    Args:\n",
    "        emb: a Tensor with shape [..., depth].\n",
    "\n",
    "    Returns:\n",
    "        a float Tensor with shape [...]. Each element is 1 if its corresponding\n",
    "        embedding vector is all zero, and is 0 otherwise.\n",
    "    \"\"\"\n",
    "    emb_sum = torch.sum(torch.abs(emb), dim=-1)\n",
    "    return (emb_sum == 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c2dae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_to_length(padding):\n",
    "    \"\"\"Calculate the length of mask based on padding.\n",
    "\n",
    "    Args:\n",
    "        padding: a Tensor with shape [..., length].\n",
    "\n",
    "    Returns:\n",
    "        a Tensor with shape [...].\n",
    "    \"\"\"\n",
    "    non_padding = 1.0 - padding\n",
    "    return torch.sum(non_padding, dim=-1).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lshgating example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a35b9633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor([\n",
    "    [0.1, -0.2, 0.3],\n",
    "    [0.1, -0.2, 0.4],\n",
    "    [-0.7, -0.8, 0.9],\n",
    "    [1.0, 1.1, 1.2]\n",
    "])\n",
    "\n",
    "#lshgating\n",
    "lsh_gating = LshGating(depth=3, nb_hyperplanes=2) \n",
    "#Use 2 HP to divide space into 4 buckets (2^2 = 4).\n",
    "\n",
    "#get ids\n",
    "bucket_ids = lsh_gating.get_gates(input_tensor)\n",
    "print(bucket_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73e1ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3fa4248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding_mask:\n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 1., 0.]])\n",
      "padding_length:\n",
      " tensor([[2, 2, 1],\n",
      "        [0, 3, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "embeddings = torch.tensor([\n",
    "    [[0.1, 0.2, 0.3], [0.2, 0.1, 0.0], [0.4, 0.5, 0.6]],\n",
    "    [[0.7, 0.8, 0.9], [0.0, 0.0, 0.0], [1.0, 1.1, 1.2]]])\n",
    "print('padding_mask:\\n',embedding_to_padding(embeddings))\n",
    "print('padding_length:\\n',padding_to_length(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f343454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b352c230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

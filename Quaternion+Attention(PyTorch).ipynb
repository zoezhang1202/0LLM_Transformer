{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09e8d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88ea74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_quaternion_mul(kernel, concat_dim=0):\n",
    "    r, i, j, k = torch.chunk(kernel, 4, dim=-1)\n",
    "    r2 = torch.cat([r, -i, -j, -k], dim=-1)\n",
    "    i2 = torch.cat([i, r, -k, j], dim=-1)\n",
    "    j2 = torch.cat([j, k, r, -i], dim=-1)\n",
    "    k2 = torch.cat([k, -j, i, r], dim=-1)\n",
    "    hamilton = torch.cat([r2, i2, j2, k2], dim=concat_dim)\n",
    "    return hamilton\n",
    "\n",
    "def get_r(x, dim=1):\n",
    "    return torch.chunk(x, 4, dim=dim)[0]\n",
    "\n",
    "def get_i(x, dim=1):\n",
    "    return torch.chunk(x, 4, dim=dim)[1]\n",
    "\n",
    "def get_j(x, dim=1):\n",
    "    return torch.chunk(x, 4, dim=dim)[2]\n",
    "\n",
    "def get_k(x, dim=1):\n",
    "    return torch.chunk(x, 4, dim=dim)[3]\n",
    "\n",
    "def quaternion_attention(a, b):\n",
    "    \"\"\"Performs dot product attention between two quaternion sequences.\"\"\"\n",
    "    print(\"light Attention!\")\n",
    "    print(a)\n",
    "    print(b)\n",
    "    \n",
    "    ar, ai, aj, ak = torch.chunk(a, 4, dim=-1)\n",
    "    br, bi, bj, bk = torch.chunk(b, 4, dim=-1)\n",
    "    \n",
    "    r = torch.matmul(ar, br.transpose(-2, -1)) - torch.matmul(ai, bi.transpose(-2, -1)) - torch.matmul(aj, bj.transpose(-2, -1)) - torch.matmul(ak, bk.transpose(-2, -1))\n",
    "    i = torch.matmul(ar, bi.transpose(-2, -1)) + torch.matmul(ai, br.transpose(-2, -1)) + torch.matmul(aj, bk.transpose(-2, -1)) - torch.matmul(ak, bj.transpose(-2, -1))\n",
    "    j = torch.matmul(ar, bj.transpose(-2, -1)) - torch.matmul(ai, bk.transpose(-2, -1)) + torch.matmul(aj, br.transpose(-2, -1)) + torch.matmul(ak, bi.transpose(-2, -1))\n",
    "    k = torch.matmul(ar, bk.transpose(-2, -1)) + torch.matmul(ai, bj.transpose(-2, -1)) - torch.matmul(aj, bi.transpose(-2, -1)) + torch.matmul(ak, br.transpose(-2, -1))\n",
    "    \n",
    "    return [r, i, j, k]\n",
    "\n",
    "def quaternion_dot_product_att(a, b):\n",
    "    \"\"\"Wrapper for two sequences.\"\"\"\n",
    "    al = a.shape[1]\n",
    "    bl = b.shape[1]\n",
    "    d = a.shape[2]\n",
    "    bsz = b.shape[0]\n",
    "    \n",
    "    a = a.view(-1, d)\n",
    "    a = a.repeat(bl, 1)\n",
    "    b = b.view(-1, d)\n",
    "    b = b.repeat(al, 1)\n",
    "    \n",
    "    att = quaternion_dot(a, b)\n",
    "    att = att.view(bsz, -1, al * bl)\n",
    "    att = torch.sum(att, dim=1)\n",
    "    \n",
    "    return att.view(-1, al * bl)\n",
    "\n",
    "def quaternion_dot_3d(q0, q1):\n",
    "    d = q0.shape[2]\n",
    "    sq = q0.shape[1]\n",
    "    \n",
    "    q0 = q0.view(-1, d)\n",
    "    q1 = q1.view(-1, d)\n",
    "    \n",
    "    out = quaternion_dot(q0, q1)\n",
    "    return out.view(-1, sq, d)\n",
    "\n",
    "def quaternion_dot(q0, q1):\n",
    "    \"\"\"Quaternion product between 2 quaternions.\"\"\"\n",
    "    q1_r = get_r(q1)\n",
    "    q1_i = get_i(q1)\n",
    "    q1_j = get_j(q1)\n",
    "    q1_k = get_k(q1)\n",
    "    \n",
    "    r_base = q0 * q1\n",
    "    r = get_r(r_base) - get_i(r_base) - get_j(r_base) - get_k(r_base)\n",
    "    \n",
    "    i_base = q0 * torch.cat([q1_i, q1_r, q1_k, q1_j], dim=1)\n",
    "    i = get_r(i_base) + get_i(i_base) + get_j(i_base) - get_k(i_base)\n",
    "    \n",
    "    j_base = q0 * torch.cat([q1_j, q1_k, q1_r, q1_i], dim=1)\n",
    "    j = get_r(j_base) - get_i(j_base) + get_j(j_base) + get_k(j_base)\n",
    "    \n",
    "    k_base = q0 * torch.cat([q1_k, q1_j, q1_i, q1_r], dim=1)\n",
    "    k = get_r(k_base) + get_i(k_base) - get_j(k_base) + get_k(k_base)\n",
    "    \n",
    "    return torch.cat([r, i, j, k], dim=1)\n",
    "\n",
    "def quaternion_concat(x, dim):\n",
    "    \"\"\"Concatenates quaternion components individually.\"\"\"\n",
    "    output = [[] for _ in range(4)]\n",
    "    for _x in x:\n",
    "        sp = torch.chunk(_x, 4, dim=dim)\n",
    "        for i in range(4):\n",
    "            output[i].append(sp[i])\n",
    "    \n",
    "    final = []\n",
    "    for o in output:\n",
    "        o = torch.cat(o, dim)\n",
    "        final.append(o)\n",
    "    \n",
    "    return torch.cat(final, dim)\n",
    "\n",
    "def quaternion_ffn_3d(x, dim, num_layers=1, activation=None):\n",
    "    \"\"\"Quaternion Feed-forward layers to 3D input [bsz x seq_len x dim].\"\"\"\n",
    "    print(\"QFFN layer..\")\n",
    "    _d = x.shape[2]\n",
    "    sq = x.shape[1]\n",
    "    \n",
    "    x = x.view(-1, _d)\n",
    "    x = quaternion_ffn(x, dim, num_layers=num_layers, activation=activation)\n",
    "    return x.view(-1, sq, dim)\n",
    "\n",
    "def quaternion_ffn(x, dim, num_layers=1, activation=None):\n",
    "    \"\"\"Implements quaternion feed-forward layer.\"\"\"\n",
    "    input_dim = x.shape[1] // 4\n",
    "    kernel = torch.nn.Parameter(torch.randn(input_dim, dim))\n",
    "    hamilton = make_quaternion_mul(kernel)\n",
    "    \n",
    "    output = torch.matmul(x, hamilton)\n",
    "    if activation:\n",
    "        output = activation(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def hamilton_product(x, kernel):\n",
    "    h = make_quaternion_mul(kernel)\n",
    "    return torch.matmul(x, h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4702cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a28e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_component(antecedent,\n",
    "                                total_depth,\n",
    "                                filter_width=1,\n",
    "                                padding=\"valid\",\n",
    "                                name=\"c\",\n",
    "                                vars_3d_num_heads=0):\n",
    "    input_depth = antecedent.size(-1)\n",
    "    initializer_stddev = input_depth ** -0.5\n",
    "    if \"q\" in name:\n",
    "        depth_per_head = total_depth\n",
    "        initializer_stddev *= depth_per_head ** -0.5\n",
    "\n",
    "    if vars_3d_num_heads > 0:\n",
    "        assert filter_width == 1\n",
    "        input_depth = antecedent.size(-1)\n",
    "        depth_per_head = total_depth // vars_3d_num_heads\n",
    "        initializer_stddev = input_depth ** -0.5\n",
    "        if \"q\" in name:\n",
    "            initializer_stddev *= depth_per_head ** -0.5\n",
    "        var = nn.Parameter(torch.randn(input_depth, vars_3d_num_heads, total_depth // vars_3d_num_heads) * initializer_stddev)\n",
    "        var = var.to(antecedent.dtype)\n",
    "        var = var.view(input_depth, total_depth)\n",
    "        return torch.einsum('bld,df->blf', antecedent, var)\n",
    "    \n",
    "    if filter_width == 1:\n",
    "        return quarternion_ffn_3d(antecedent, total_depth, name=name,\n",
    "                                  init=torch.nn.init.normal_(torch.empty(input_depth, total_depth), mean=0, std=initializer_stddev))\n",
    "    else:\n",
    "        return F.conv1d(antecedent.permute(0, 2, 1), \n",
    "                        torch.randn(total_depth, input_depth, filter_width) * initializer_stddev, \n",
    "                        padding=padding).permute(0, 2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_qkv(query_antecedent,\n",
    "                memory_antecedent,\n",
    "                total_key_depth,\n",
    "                total_value_depth,\n",
    "                q_filter_width=1,\n",
    "                kv_filter_width=1,\n",
    "                q_padding=\"valid\",\n",
    "                kv_padding=\"valid\",\n",
    "                vars_3d_num_heads=0):\n",
    "    if memory_antecedent is None:\n",
    "        memory_antecedent = query_antecedent\n",
    "    q = compute_attention_component(\n",
    "        query_antecedent,\n",
    "        total_key_depth,\n",
    "        q_filter_width,\n",
    "        q_padding,\n",
    "        \"q\",\n",
    "        vars_3d_num_heads=vars_3d_num_heads)\n",
    "    k = compute_attention_component(\n",
    "        memory_antecedent,\n",
    "        total_key_depth,\n",
    "        kv_filter_width,\n",
    "        kv_padding,\n",
    "        \"k\",\n",
    "        vars_3d_num_heads=vars_3d_num_heads)\n",
    "    v = compute_attention_component(\n",
    "        memory_antecedent,\n",
    "        total_value_depth,\n",
    "        kv_filter_width,\n",
    "        kv_padding,\n",
    "        \"v\",\n",
    "        vars_3d_num_heads=vars_3d_num_heads)\n",
    "    return q, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, num_heads):\n",
    "    batch_size, length, depth = x.size()\n",
    "    depth_per_head = depth // num_heads\n",
    "    x = x.view(batch_size, length, num_heads, depth_per_head)\n",
    "    return x.permute(0, 2, 1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f243ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_heads(x):\n",
    "    batch_size, num_heads, length, depth_per_head = x.size()\n",
    "    return x.permute(0, 2, 1, 3).contiguous().view(batch_size, length, num_heads * depth_per_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d31c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention(q, k, v, bias, dropout_rate=0.0, image_shapes=None,\n",
    "                          save_weights_to=None, make_image_summary=True,\n",
    "                          dropout_broadcast_dims=None):\n",
    "    logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    if bias is not None:\n",
    "        logits += bias\n",
    "    weights = F.softmax(logits, dim=-1)\n",
    "    if dropout_rate > 0.0:\n",
    "        weights = F.dropout(weights, p=dropout_rate)\n",
    "    return torch.matmul(weights, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823fc16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_dot_product_attention(q, k, v, bias, dropout_rate=0.0, image_shapes=None,\n",
    "                                     save_weights_to=None, make_image_summary=True,\n",
    "                                     dropout_broadcast_dims=None):\n",
    "    print(\"Using QDP attention..\")\n",
    "    logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    if bias is not None:\n",
    "        logits += bias\n",
    "    weights = F.softmax(logits, dim=-1)\n",
    "    if dropout_rate > 0.0:\n",
    "        weights = F.dropout(weights, p=dropout_rate)\n",
    "    return torch.matmul(weights, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f0ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def quarternion_ffn_3d(x, output_depth, name='output_transform', init=None):\n",
    "#    return F.linear(x, init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc886ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(query_antecedent,\n",
    "                        memory_antecedent,\n",
    "                        bias,\n",
    "                        total_key_depth,\n",
    "                        total_value_depth,\n",
    "                        output_depth,\n",
    "                        num_heads,\n",
    "                        dropout_rate,\n",
    "                        attention_type=\"dot_product\",\n",
    "                        max_relative_position=None,\n",
    "                        heads_share_relative_embedding=False,\n",
    "                        add_relative_to_values=False,\n",
    "                        image_shapes=None,\n",
    "                        block_length=128,\n",
    "                        block_width=128,\n",
    "                        q_filter_width=1,\n",
    "                        kv_filter_width=1,\n",
    "                        q_padding=\"valid\",\n",
    "                        kv_padding=\"valid\",\n",
    "                        cache=None,\n",
    "                        gap_size=0,\n",
    "                        num_memory_blocks=2,\n",
    "                        name=\"multihead_attention\",\n",
    "                        save_weights_to=None,\n",
    "                        make_image_summary=True,\n",
    "                        dropout_broadcast_dims=None,\n",
    "                        vars_3d=False,\n",
    "                        is_training=False,\n",
    "                        **kwargs):\n",
    "    if total_key_depth % num_heads != 0:\n",
    "        raise ValueError(\"Key depth (%d) must be divisible by the number of attention heads (%d).\" % (total_key_depth, num_heads))\n",
    "    if total_value_depth % num_heads != 0:\n",
    "        raise ValueError(\"Value depth (%d) must be divisible by the number of attention heads (%d).\" % (total_value_depth, num_heads))\n",
    "\n",
    "    vars_3d_num_heads = num_heads if vars_3d else 0\n",
    "    \n",
    "    if cache is None or memory_antecedent is None:\n",
    "        q, k, v = compute_qkv(query_antecedent, memory_antecedent,\n",
    "                              total_key_depth, total_value_depth, q_filter_width,\n",
    "                              kv_filter_width, q_padding, kv_padding,\n",
    "                              vars_3d_num_heads=vars_3d_num_heads)\n",
    "    if cache is not None:\n",
    "        if attention_type not in [\"dot_product\", \"dot_product_relative\", \"quaternion_dot_product\"]:\n",
    "            raise NotImplementedError(\"Caching is not guaranteed to work with attention types other than dot_product.\")\n",
    "        if bias is None:\n",
    "            raise ValueError(\"Bias required for caching. See function docstring for details.\")\n",
    "\n",
    "        if memory_antecedent is not None:\n",
    "            q = compute_attention_component(query_antecedent, total_key_depth,\n",
    "                                            q_filter_width, q_padding, \"q\",\n",
    "                                            vars_3d_num_heads=vars_3d_num_heads)\n",
    "            k = cache[\"k_encdec\"]\n",
    "            v = cache[\"v_encdec\"]\n",
    "        else:\n",
    "            k = split_heads(k, num_heads)\n",
    "            v = split_heads(v, num_heads)\n",
    "            decode_loop_step = kwargs.get(\"decode_loop_step\")\n",
    "            if decode_loop_step is None:\n",
    "                k = cache[\"k\"] = torch.cat([cache[\"k\"], k], dim=2)\n",
    "                v = cache[\"v\"] = torch.cat([cache[\"v\"], v], dim=2)\n",
    "            else:\n",
    "                tmp_k = cache[\"k\"].permute(2, 0, 1, 3)\n",
    "                tmp_k[decode_loop_step] = k.squeeze(2)\n",
    "                k = cache[\"k\"] = tmp_k.permute(1, 2, 0, 3)\n",
    "                tmp_v = cache[\"v\"].permute(2, 0, 1, 3)\n",
    "                tmp_v[decode_loop_step] = v.squeeze(2)\n",
    "                v = cache[\"v\"] = tmp_v.permute(1, 2, 0, 3)\n",
    "\n",
    "    q = split_heads(q, num_heads)\n",
    "    if cache is None:\n",
    "        k = split_heads(k, num_heads)\n",
    "        v = split_heads(v, num_heads)\n",
    "\n",
    "    key_depth_per_head = total_key_depth // num_heads\n",
    "    if not vars_3d:\n",
    "        q *= key_depth_per_head**-0.5\n",
    "\n",
    "    additional_returned_value = None\n",
    "    if callable(attention_type):\n",
    "        x = attention_type(q, k, v, **kwargs)\n",
    "        if isinstance(x, tuple):\n",
    "            x, additional_returned_value = x\n",
    "    elif attention_type == \"dot_product\":\n",
    "        x = dot_product_attention(q, k, v, bias, dropout_rate, image_shapes,\n",
    "                                  save_weights_to=save_weights_to,\n",
    "                                  make_image_summary=make_image_summary,\n",
    "                                  dropout_broadcast_dims=dropout_broadcast_dims)\n",
    "    elif attention_type == 'quaternion_dot_product':\n",
    "        print(\"Using QDP attention..\")\n",
    "        x = quaternion_dot_product_attention(q, k, v, bias, dropout_rate, image_shapes,\n",
    "                                             save_weights_to=save_weights_to,\n",
    "                                             make_image_summary=make_image_summary,\n",
    "                                             dropout_broadcast_dims=dropout_broadcast_dims)\n",
    "\n",
    "    x = combine_heads(x)\n",
    "    x = quarternion_ffn_3d(x, output_depth, name='output_transform',\n",
    "                           init=torch.nn.init.normal_(torch.empty(output_depth, output_depth), mean=0, std=output_depth ** -0.5))\n",
    "\n",
    "    if additional_returned_value is not None:\n",
    "        return x, additional_returned_value\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd41240e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using QDP attention..\n",
      "Using QDP attention..\n",
      "torch.Size([1, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Testing the updated multihead_attention function\n",
    "query_antecedent = torch.randn(1, 2, 4)\n",
    "memory_antecedent = torch.randn(1, 8, 4)\n",
    "bias = None\n",
    "total_key_depth = 4\n",
    "total_value_depth = 4\n",
    "output_depth = 4\n",
    "num_heads = 2\n",
    "dropout_rate = 0.1\n",
    "attention_type = \"quaternion_dot_product\"\n",
    "vars_3d = False\n",
    "\n",
    "output = multihead_attention(query_antecedent, memory_antecedent, bias,\n",
    "                             total_key_depth, total_value_depth, output_depth,\n",
    "                             num_heads, dropout_rate, attention_type, vars_3d=vars_3d)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5081b4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2630, -0.1027, -0.0266, -0.0309],\n",
       "         [ 0.3399,  0.1480,  0.0696,  0.0332]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

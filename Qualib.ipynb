{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ffce855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fbef1ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input tensors\n",
    "a = torch.randn(2, 3, 4)  # Batch size 2, sequence length 3, quaternion components 4 \n",
    "b = torch.randn(2, 3, 4)\n",
    "x = torch.randn(2, 3, 4)  # Input for FFN\n",
    "dim = 4  # Dimension of the feed-forward network\n",
    "num_layers = 2  # Number of layers in FFN\n",
    "activation = F.relu  # Activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2936d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_quaternion_mul(kernel, concat_dim=0):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    r, i, j, k = torch.chunk(kernel, 4, dim=-1)\n",
    "    r2 = torch.cat([r, -i, -j, -k], dim=-1)\n",
    "    i2 = torch.cat([i, r, -k, j], dim=-1)\n",
    "    j2 = torch.cat([j, k, r, -i], dim=-1)\n",
    "    k2 = torch.cat([k, -j, i, r], dim=-1)\n",
    "    hamilton = torch.cat([r2, i2, j2, k2], dim=concat_dim)\n",
    "    return hamilton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f671800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamilton product kernel:\n",
      " tensor([[-0.6688,  1.0368, -2.5359,  0.2173],\n",
      "        [-1.0368, -0.6688,  0.2173,  2.5359],\n",
      "        [ 2.5359, -0.2173, -0.6688,  1.0368],\n",
      "        [-0.2173, -2.5359, -1.0368, -0.6688]])\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Example for make_quaternion_mul\n",
    "kernel = torch.randn(1, 4)\n",
    "hamilton = make_quaternion_mul(kernel)\n",
    "print(\"Hamilton product kernel:\\n\", hamilton)\n",
    "print(hamilton.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b91597c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r(x, dim=1):\n",
    "    return torch.chunk(x, 4, dim=dim)[0]\n",
    "\n",
    "def get_i(x, dim=1):\n",
    "    return torch.chunk(x, 4, dim=dim)[1]\n",
    "\n",
    "def get_j(x, dim=1):\n",
    "    return torch.chunk(x, 4, dim=dim)[2]\n",
    "\n",
    "def get_k(x, dim=1):\n",
    "    return torch.chunk(x, 4, dim=dim)[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab11f59b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "836ac4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_attention(a, b):\n",
    "    \"\"\"Performs dot product attention between two quaternion sequences.\"\"\"\n",
    "    print(\"light Attention!\")\n",
    "    print(a)\n",
    "    print(b)\n",
    "    \n",
    "    ar, ai, aj, ak = torch.chunk(a, 4, dim=-1)\n",
    "    br, bi, bj, bk = torch.chunk(b, 4, dim=-1)\n",
    "    \n",
    "    r = torch.matmul(ar, br.transpose(-2, -1)) - torch.matmul(ai, bi.transpose(-2, -1)) - torch.matmul(aj, bj.transpose(-2, -1)) - torch.matmul(ak, bk.transpose(-2, -1))\n",
    "    i = torch.matmul(ar, bi.transpose(-2, -1)) + torch.matmul(ai, br.transpose(-2, -1)) + torch.matmul(aj, bk.transpose(-2, -1)) - torch.matmul(ak, bj.transpose(-2, -1))\n",
    "    j = torch.matmul(ar, bj.transpose(-2, -1)) - torch.matmul(ai, bk.transpose(-2, -1)) + torch.matmul(aj, br.transpose(-2, -1)) + torch.matmul(ak, bi.transpose(-2, -1))\n",
    "    k = torch.matmul(ar, bk.transpose(-2, -1)) + torch.matmul(ai, bj.transpose(-2, -1)) - torch.matmul(aj, bi.transpose(-2, -1)) + torch.matmul(ak, br.transpose(-2, -1))\n",
    "    \n",
    "    return [r, i, j, k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7b9f85c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_dot_product_att(a, b):\n",
    "    \"\"\"Wrapper for two sequences.\"\"\"\n",
    "    al = a.shape[1]\n",
    "    bl = b.shape[1]\n",
    "    d = a.shape[2]\n",
    "    bsz = b.shape[0]\n",
    "    \n",
    "    a = a.view(-1, d)\n",
    "    a = a.repeat(bl, 1)\n",
    "    b = b.view(-1, d)\n",
    "    b = b.repeat(al, 1)\n",
    "    \n",
    "    att = quaternion_dot(a, b)\n",
    "    att = att.view(bsz, -1, al * bl)\n",
    "    att = torch.sum(att, dim=1)\n",
    "    \n",
    "    return att.view(-1, al * bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b904f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_dot_3d(q0, q1):\n",
    "    d = q0.shape[2]\n",
    "    sq = q0.shape[1]\n",
    "    \n",
    "    q0 = q0.view(-1, d)\n",
    "    q1 = q1.view(-1, d)\n",
    "    \n",
    "    out = quaternion_dot(q0, q1)\n",
    "    return out.view(-1, sq, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7420b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_dot(q0, q1):\n",
    "    \"\"\"Quaternion product between 2 quaternions.\"\"\"\n",
    "    q1_r = get_r(q1)\n",
    "    q1_i = get_i(q1)\n",
    "    q1_j = get_j(q1)\n",
    "    q1_k = get_k(q1)\n",
    "    \n",
    "    r_base = q0 * q1\n",
    "    r = get_r(r_base) - get_i(r_base) - get_j(r_base) - get_k(r_base)\n",
    "    \n",
    "    i_base = q0 * torch.cat([q1_i, q1_r, q1_k, q1_j], dim=1)\n",
    "    i = get_r(i_base) + get_i(i_base) + get_j(i_base) - get_k(i_base)\n",
    "    \n",
    "    j_base = q0 * torch.cat([q1_j, q1_k, q1_r, q1_i], dim=1)\n",
    "    j = get_r(j_base) - get_i(j_base) + get_j(j_base) + get_k(j_base)\n",
    "    \n",
    "    k_base = q0 * torch.cat([q1_k, q1_j, q1_i, q1_r], dim=1)\n",
    "    k = get_r(k_base) + get_i(k_base) - get_j(k_base) + get_k(k_base)\n",
    "    \n",
    "    return torch.cat([r, i, j, k], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ba65887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_concat(x, dim):\n",
    "    \"\"\"Concatenates quaternion components individually.\"\"\"\n",
    "    output = [[] for _ in range(4)]\n",
    "    for _x in x:\n",
    "        sp = torch.chunk(_x, 4, dim=dim)\n",
    "        for i in range(4):\n",
    "            output[i].append(sp[i])\n",
    "    \n",
    "    final = []\n",
    "    for o in output:\n",
    "        o = torch.cat(o, dim)\n",
    "        final.append(o)\n",
    "    \n",
    "    return torch.cat(final, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30f9760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_ffn_3d(x, dim, num_layers=1, activation=None):\n",
    "    \"\"\"Quaternion Feed-forward layers to 3D input [bsz x seq_len x dim].\"\"\"\n",
    "    print(\"QFFN layer..\")\n",
    "    _d = x.shape[2]\n",
    "    sq = x.shape[1]\n",
    "    \n",
    "    x = x.view(-1, _d)\n",
    "    x = quaternion_ffn(x, dim, num_layers=num_layers, activation=activation)\n",
    "    return x.view(-1, sq, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cfbfe34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_ffn(x, dim, num_layers=1, activation=None):\n",
    "    \"\"\"Implements quaternion feed-forward layer.\"\"\"\n",
    "    input_dim = x.shape[1] // 4\n",
    "    kernel = torch.nn.Parameter(torch.randn(input_dim, dim))\n",
    "    hamilton = make_quaternion_mul(kernel)\n",
    "    \n",
    "    output = torch.matmul(x, hamilton)\n",
    "    if activation:\n",
    "        output = activation(output)\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5dd78ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamilton_product(x, kernel):\n",
    "    h = make_quaternion_mul(kernel)\n",
    "    return torch.matmul(x, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9b92cabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QFFN layer..\n",
      "Input:\n",
      "tensor([[[-0.3506,  0.0970,  1.3514, -0.6169,  1.3352, -1.4132,  0.1767,\n",
      "           0.8505],\n",
      "         [ 0.7452,  0.0193,  0.2961, -0.2805, -0.7586,  0.3983, -1.3985,\n",
      "          -0.0962],\n",
      "         [-0.0128, -2.1986, -0.5678,  0.4595, -0.6890,  0.6323,  1.1189,\n",
      "           1.2924],\n",
      "         [-0.1032,  2.8755, -0.7394,  0.2886, -0.1981,  0.4650, -0.5502,\n",
      "          -0.4260],\n",
      "         [ 0.1850, -0.4664,  2.1679,  0.5651, -1.1531, -0.7372, -0.4274,\n",
      "           1.3448]],\n",
      "\n",
      "        [[ 0.3386,  0.9734,  1.2382,  0.6949, -1.7217, -0.8094,  0.1479,\n",
      "          -2.2291],\n",
      "         [ 1.9987,  0.8493,  0.2982,  2.4384, -0.3508,  0.6973,  1.3037,\n",
      "           1.2777],\n",
      "         [ 0.4218, -0.4563,  0.1082, -1.4774,  0.2493,  0.1442,  1.3681,\n",
      "          -0.6411],\n",
      "         [ 0.2299,  0.1423,  0.8334, -0.8379, -0.0913, -1.3382,  0.0513,\n",
      "          -0.6998],\n",
      "         [-1.6995,  1.1139,  0.8475,  0.3015, -0.5085, -1.4477,  0.6160,\n",
      "          -0.9741]]])\n",
      "\n",
      "FFN Output:\n",
      "tensor([[[ 3.5576, -2.2303, -2.2177, -0.6984,  4.1498,  0.7500,  4.1415,\n",
      "           2.4356],\n",
      "         [-1.6703,  0.5222,  3.9704,  2.0037, -2.1800,  2.5788,  0.8648,\n",
      "          -3.0112],\n",
      "         [ 0.8952,  0.9659,  1.3536, -0.7100,  1.2421, -3.9305, -0.6631,\n",
      "           2.7476],\n",
      "         [-2.1581,  2.3128, -2.4829,  0.7537, -1.3947,  1.0837, -5.1116,\n",
      "          -3.4578],\n",
      "         [ 0.6694, -3.3542,  0.9901,  2.4167,  2.3180, -1.7181,  6.5147,\n",
      "          -2.4168]],\n",
      "\n",
      "        [[-5.5888, -3.7327, -2.5064,  2.3210, -3.6142, -2.0577,  5.2086,\n",
      "          -5.7708],\n",
      "         [ 2.9529,  1.9729, -5.6261,  6.8291, -5.1659, -5.1770, -1.0427,\n",
      "           1.0628],\n",
      "         [-2.6124,  0.0230, -1.8946,  0.3151, -0.0448, -0.8258,  0.9323,\n",
      "           2.3397],\n",
      "         [-1.9802, -1.5390, -1.3314, -0.0589,  0.4453,  0.3196,  4.4125,\n",
      "          -1.2525],\n",
      "         [-2.2481, -4.1236, -4.0526, -3.0080,  3.8351, -2.4219,  2.9700,\n",
      "          -2.7001]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "batch_size = 2\n",
    "seq_length = 5\n",
    "embed_dim = 8\n",
    "\n",
    "# Create random input tensor (batch_size, seq_length, embed_dim)\n",
    "x = torch.randn(batch_size, seq_length, embed_dim)\n",
    "\n",
    "# Apply quaternion feed-forward network\n",
    "ffn_output = quaternion_ffn_3d(x, dim=embed_dim)\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(\"\\nFFN Output:\")\n",
    "print(ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e472ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
